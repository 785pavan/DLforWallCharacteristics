{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard module is not an IPython extension.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division, absolute_import, unicode_literals\n",
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=True, \n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 17 02:15:29 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  On   | 00000000:05:00.0 Off |                  N/A |\n",
      "| 23%   39C    P2    57W / 250W |  11082MiB / 11177MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  On   | 00000000:06:00.0 Off |                  N/A |\n",
      "| 23%   36C    P8    10W / 250W |    159MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  On   | 00000000:09:00.0 Off |                  N/A |\n",
      "| 23%   35C    P8     9W / 250W |    159MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  On   | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 23%   30C    P8     9W / 250W |    159MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     22029      C   /usr/bin/python3                             137MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import ImageOps, Image\n",
    "from sklearn.cluster import KMeans\n",
    "from time import time\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from tensorflow.keras.layers import Lambda, RepeatVector, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalMaxPool2D\n",
    "from tensorflow.keras.layers import concatenate, add\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "print('setup finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting perameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_width = 128\n",
    "im_height = 128\n",
    "path_train = r'/data/student/training/'\n",
    "classes = ['dark_dense_distribution', 'Elongated_nuclei','light_dense_istribution','light_distribution',\n",
    "           'No_nuclei_with_uniform_texture','No_nuclei_with_unstructured','Red_texture',  'Round_nuclei','Mixed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/student/training/images'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train + 'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode( mask, palette):\n",
    "        \"\"\"\n",
    "        Converts mask to a one-hot encoding specified by the semantic map.\n",
    "        \"\"\"\n",
    "        one_hot_map = []\n",
    "        for colour in palette:\n",
    "            class_map = tf.reduce_all(tf.equal(mask, colour), axis=-1)\n",
    "            one_hot_map.append(class_map)\n",
    "        one_hot_map = tf.stack(one_hot_map, axis=-1)\n",
    "        one_hot_map = tf.cast(one_hot_map, tf.float32)\n",
    "        \n",
    "        return one_hot_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_maker(savedir, path, filename, target_size=(256,256)):\n",
    "    \n",
    "    '''opens one images at a time and saves them into patches of given hight and width. \n",
    "    It also handels RGBA format issues'''\n",
    "    if not os.path.isdir(savedir):\n",
    "            os.mkdir(savedir)\n",
    "    img = Image.open(path + filename)\n",
    "    width, height = img.size\n",
    "\n",
    "    start_pos = start_x, start_y = (0, 0)\n",
    "    cropped_image_size = w, h = target_size\n",
    "\n",
    "    new_name = filename.split('.')[0]\n",
    "    frame_num = 1\n",
    "    for col_i in tqdm_notebook(range(0, width, w)):\n",
    "        for row_i in range(0, height, h):\n",
    "            crop = img.crop((col_i, row_i, col_i + w, row_i + h))\n",
    "            save_to= os.path.join(savedir, new_name + \"_{:04}.jpg\")\n",
    "            if crop.mode in ('RGBA', 'LA'):\n",
    "                background = Image.new(crop.mode[:-1], crop.size, (255,255,255))\n",
    "                background.paste(crop, crop.split()[-1])\n",
    "                crop = background\n",
    "            crop.save(save_to.format(frame_num))\n",
    "            frame_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patches(savedir, path, images, masks):\n",
    "    if not os.path.isdir(savedir):\n",
    "            os.mkdir(savedir)\n",
    "    path_im = path + images \n",
    "    path_ms = path + masks\n",
    "    ids = next(os.walk(path_ms))[2]\n",
    "    for n, id_ in tqdm_notebook(enumerate(ids), total=len(ids)):\n",
    "        '''image pathing'''\n",
    "        print(id_.replace('.png', '.tif'))\n",
    "        patch_maker(savedir + 'images', path_im, id_.replace('.png', '.tif'))\n",
    "        \n",
    "        '''mask patching'''\n",
    "        patch_maker(savedir + 'masks', path_ms, id_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args = dict(rotation_range=20,\n",
    "                    width_shift_range=0.01,\n",
    "                    height_shift_range=0.01,\n",
    "                    shear_range=0.01,\n",
    "                    zoom_range=0.01,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator(batch_size,train_path,aug_dict=None,target_size = (im_height,im_width),seed = 42):\n",
    "    '''\n",
    "    can generate image and mask at the same time\n",
    "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
    "    '''\n",
    "    if aug_dict: \n",
    "        image_datagen = ImageDataGenerator(**aug_dict)\n",
    "        mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    else:\n",
    "        image_datagen = ImageDataGenerator()\n",
    "        mask_datagen = ImageDataGenerator()\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path + 'images/',\n",
    "        class_mode = None,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path + 'masks/',\n",
    "        class_mode = None,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "    return  (pair for pair in zip(image_generator, mask_generator))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37228 images belonging to 1 classes.\n",
      "Found 37228 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = trainGenerator(8, path_train + '/patches/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_colors = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprossesing_image(image, is_image_name=True):\n",
    "    ''' takes either image filename or file itself and returns a ndarray and width and height\n",
    "    \n",
    "    @params: image = filename or image\n",
    "             is_image_name = True is its a filename or \n",
    "                             False if passing image directly'''\n",
    "    if is_image_name:\n",
    "        image = Image.open(image)\n",
    "    if not type(image).__module__ == np.__name__:\n",
    "        if image.mode in ('RGBA', 'LA'):\n",
    "            background = Image.new(image.mode[:-1], image.size, (255,255,255))\n",
    "            background.paste(image, image.split()[-1])\n",
    "            image = background\n",
    "        image = np.array(image, dtype=np.float64)/255.\n",
    "    w, h, d = original_shape = image.shape\n",
    "    assert d == 3\n",
    "    image_array = np.reshape(image, (w*h, d))\n",
    "    return image_array, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_masks = r'/data/student/training/patches/masks/masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_label(labels, w, h):\n",
    "    ''' takes 1d array of labels and reshapes it into orignal image height and width\n",
    "    @params: labels = label\n",
    "             w = width of image\n",
    "             h = height of image'''\n",
    "    \n",
    "    lbl_reshaped = np.zeros((w, h, 1))\n",
    "    labels_idx = 0\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            lbl_reshaped[i][j] = labels[labels_idx]\n",
    "            labels_idx += 1\n",
    "    return lbl_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 12.791s. \n",
      "[[9.99969192e-01 9.99974928e-01 9.99938496e-01]\n",
      " [9.99935247e-01 4.15438431e-01 7.05848094e-01]\n",
      " [5.01991529e-01 5.01991529e-01 5.01991529e-01]\n",
      " [9.99873380e-01 9.99968000e-01 3.90933333e-03]\n",
      " [1.73866668e-04 1.57992158e-04 9.95901051e-01]\n",
      " [5.01959467e-01 9.42431384e-05 1.74431373e-04]\n",
      " [6.41254916e-05 5.01975718e-01 3.96994510e-03]\n",
      " [9.99936878e-01 3.13098051e-05 1.64645333e-01]\n",
      " [5.05913537e-01 3.13725502e-05 4.98070463e-01]\n",
      " [9.99937380e-01 6.47121129e-01 3.31294119e-05]]\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "color_palette_2, w, h = preprossesing_image(r'/data/student/github/DLforWallCharacteristics/color_palette_2.jpg')\n",
    "kmeans_color_palette_2 = KMeans(n_clusters=n_colors, random_state=42).fit(color_palette_2)\n",
    "print('done in %0.3fs. ' % (time() - t0))\n",
    "print(kmeans_color_palette_2.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_with_labels(labels, w, h, centers):\n",
    "    ''' convert image back into rgb format from labels\n",
    "    @params: labels = w x h x 1 ndarray\n",
    "             centers = cookbook for reference '''\n",
    "    image = np.zeros((w, h, 3))\n",
    "    for i, row in enumerate(labels):\n",
    "        for j, col in enumerate(row):\n",
    "            image[i][j] = centers[int(col)]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(image, is_image_name=True):\n",
    "    ''' converts image into labels using pretrained kmeans algorithm'''\n",
    "    image_array, w, h = preprossesing_image(image, is_image_name=is_image_name)\n",
    "    label = reshape_label(kmeans_color_palette_2.predict(image_array), w, h)\n",
    "    return label, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_lbl,w,h = get_labels(\"/data/student/training/testMa/patches/S_385 (26)_0100.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(label, w = None, h= None, cookbook=kmeans_color_palette_2.cluster_centers_):\n",
    "    ''' coverts labels back to image'''\n",
    "    '''if w==None or h==None:\n",
    "        w, h, _ = label.shape'''\n",
    "    image = display_image_with_labels(label, w, h, cookbook)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.uint8(kmeans_color_palette_2.cluster_centers_*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[254, 254, 254],\n",
       "       [254, 105, 179],\n",
       "       [128, 128, 128],\n",
       "       [254, 254,   0],\n",
       "       [  0,   0, 253],\n",
       "       [127,   0,   0],\n",
       "       [  0, 128,   1],\n",
       "       [254,   0,  41],\n",
       "       [129,   0, 127],\n",
       "       [254, 165,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices = np.random.randint(0, len(colors), size= (5,5))\n",
    "plt.imshow(colors[indices]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_set(train_gen = train_gen):\n",
    "    for images, masks in train_gen:\n",
    "        new_lables = []\n",
    "        for mask in masks:\n",
    "            label, w, h = get_labels(mask, is_image_name=False)\n",
    "            new_lables.append(label)\n",
    "        yield(images, np.asarray(new_lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_gen = create_mask_set()\n",
    "def plot_img2(ix):\n",
    "    x, y = next(lab_gen)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "    ax[0].imshow(((x[0]).astype(np.uint8)))\n",
    "    ax[0].set_title('Image')\n",
    "\n",
    "    ax[1].imshow((get_image(y[0], w = y[0].shape[0], h = y[0].shape[1])))\n",
    "    ax[1].set_title('mask');\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
    "    # first layer\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\")(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # second layer\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\")(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark = 15\n",
    "file_name = 'model-unet-mark_{:03}.h5'.format(mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(input_img, n_filters=16, dropout=0.5, batchnorm=True):\n",
    "    # contracting path\n",
    "    \n",
    "    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "    p1 = Dropout(dropout*0.5)(p1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    # expansive path\n",
    "    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u9 = Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='softmax') (c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1e-5\n",
    "    \n",
    "    y_true = tf.round(tf.reshape(y_true, [-1]))\n",
    "    y_pred = tf.round(tf.reshape(y_pred, [-1]))\n",
    "    \n",
    "    isct = tf.reduce_sum(y_true * y_pred)\n",
    "    \n",
    "    return 2 * isct / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred))\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input((im_height, im_width, 3), name='img')\n",
    "model = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=False)\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "callbacks = [\n",
    "    #EarlyStopping( patience=10, verbose=1),\n",
    "    ReduceLROnPlateau( factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(file_name, verbose=1, save_best_only=True, save_weights_only=True),\n",
    "    TensorBoard(logdir, histogram_freq=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
